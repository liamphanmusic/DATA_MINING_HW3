---
title: "HOMEWORK 1 - Creating Value Through Data Mining (S402010)"
author: "Liam Phan"
date: "`r Sys.Date()`"
output:
  rmdformats::material :
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: true
    highlight: tango
---

# <span style="color: #1c6155;">Quick Start</span> 

> Most of the plots are interactive, you can click or zoom to get more details ! Also don't hesitate to click on plots, they will zoom automatically ! 

## Loading Packages

```{r 2, warning=FALSE, message=FALSE }

library(data.table) # Efficient Dataframe 
library(lubridate) # For Dates 
library(tidyverse) # Multiple Package for Useful Data wrangling
library(esquisse) # Intuitive plotting
library(plyr) # Data splitting
library(ggplot2) # Plot Graphs
library(naniar) # for NA exploration in Dataframe
library(sp) # spatial data
library(plotly) # Make ggplot2 Dynamic
library(gissr) # Spatial Transformations
library(leaflet) # For Map
library(leaflet.providers) # For Custom Icons
library(geosphere) # Spatial Calculations
library(DT) # Render Table in a explorable UI
library(gridExtra) # Multiple Plot at once
library(corrplot) # Correlation Plot
library(RColorBrewer) # For Color Palette
library(rmdformats) # Theme of HTML
library(manipulateWidget) # Handling multiple plotly graphs

```

> Those are required packages

> Geosphere: Spherical trigonometry for geographic applications. That is, compute distances and related measures for angular (longitude/latitude) locations.

> Gissr: gissr is a collection of R functions which make working with spatial data easier.


# <span style="color: #1c6155;">Ex 3.4</span> 

```{r clean, include=FALSE}

rm(list = ls()) # clean environment

```

## Loading Datas and Cleaning

> Loading the dataset called "LaptopSales_red.csv" given for the Homework

```{r 3,echo=FALSE, warning=FALSE, comment=FALSE}

Laptop_Sales_Data <- fread("DATA/LaptopSales_red.csv")

#is.data.table(Laptop_Sales_Data)

#summary(Laptop_Sales_Data)

str(Laptop_Sales_Data)

```

<center>

```{r 333, echo=FALSE, warning=FALSE, comment=FALSE}

# Miss Variables Plot for the Dataset

gg_miss_var(Laptop_Sales_Data, show_pct = TRUE)

```


</center>

> Retail Price is the only variable missing at rate of approximately 4.5% 



## a.Price Questions:

```{r 4, include=FALSE}

#### Set Up a Data Subset and NA OMIT

Retail_Price_and_Dates <- Laptop_Sales_Data[,.(Retail.Price,Date)][,Date:=mdy_hm(Date)]

Retail_Price_and_Dates <- na.omit(Retail_Price_and_Dates)

```


### i. At What Price are the laptops actually selling ?

<center>

```{r 5, echo=FALSE, warning=FALSE,message=FALSE}

# Histogram of the Retail Price of Computer In 2018

ggplotly(
ggplot(Retail_Price_and_Dates) +
 aes(x = Retail.Price) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 labs(x = "Price", y = "Frequency", title = "Histogram of the Retail Price of Computer", subtitle = "In 2018") + theme_classic() + geom_vline(aes(xintercept = median(Retail_Price_and_Dates$Retail.Price)),col='black',size=2) + annotate("text",                        # Add text for mean
           x =  700,
           y =  15000,
           label = paste("Median =", median(Retail_Price_and_Dates$Retail.Price)),
           col = "black",
           size = 6)
)

```

</center>


> This Histogram shows the most frequent retail prices for all stores in 2018. In Black is the median



<center>

```{r 51, echo=FALSE, warning=FALSE, comment=FALSE}

# Boxplot of the Retail Price of Computer In 2018

ggplotly(
ggplot(Retail_Price_and_Dates) +
 aes(x = "", y = Retail.Price) +
 geom_boxplot(fill = "#1c6155") + stat_summary(fun=mean, geom="point", shape=20, size=8, color="white", fill="white") + 
 labs(y = "Price", x="",
 title = "Boxplot of the Retail Price of Computer", subtitle = "In 2018") +
 theme_classic()
)

```


> We can interpret this boxplot as the mean or median retail price of the 2018 Computer Dataset, click on the white sphere to get the mean !

</center>

```{r 52, include=FALSE}

# Actual price

Max_Date_Retail <- max(Retail_Price_and_Dates$Date)

Actual_Price <- Retail_Price_and_Dates[Date %in% Max_Date_Retail, ]

```

```{r 53,echo=FALSE}

# Print Last Recorded Prices (according to last day)

result1 <- print(paste("Last Recorded Prices are", Actual_Price[1,1], "", "and", Actual_Price[2,1],"","on the same Day with a mean of",mean(Actual_Price$Retail.Price),""))

```

> Here is given the last recorded prices for 2018


### ii. Does price change with time?


```{r 61, include=FALSE}

# Aggregating by Month
Retail_Price_and_Dates_Month <- Retail_Price_and_Dates[, mean(Retail.Price), by = floor_date(Date,unit="month")]

colnames(Retail_Price_and_Dates_Month)[1] <- "Date"
colnames(Retail_Price_and_Dates_Month)[2] <- "Mean_Retail_Price"

# Aggregating by Week
Retail_Price_and_Dates_Week <- Retail_Price_and_Dates[, mean(Retail.Price), by = floor_date(Date,unit = "week")]

colnames(Retail_Price_and_Dates_Week)[1] <- "Date"
colnames(Retail_Price_and_Dates_Week)[2] <- "Mean_Retail_Price"

# Aggregating by Day
Retail_Price_and_Dates_Day <- Retail_Price_and_Dates[, mean(Retail.Price), by = floor_date(Date,unit = "day")]

colnames(Retail_Price_and_Dates_Day)[1] <- "Date"
colnames(Retail_Price_and_Dates_Day)[2] <- "Mean_Retail_Price"

# Aggregating by Weekday

Retail_Price_and_Dates_WeekDay <- Retail_Price_and_Dates[,Weekday:=weekdays(Date)]
Retail_Price_and_Dates_WeekDay <- Retail_Price_and_Dates_WeekDay[,mean(Retail.Price),by=Weekday]

Retail_Price_and_Dates_WeekDay <- Retail_Price_and_Dates_WeekDay %>% mutate(Translation=c("Saturday","Friday","Wednesday","Monday","Tuesday","Thursday","Sunday"))

Retail_Price_and_Dates_WeekDay$Translation <- factor(Retail_Price_and_Dates_WeekDay$Translation, ordered = TRUE, levels=(c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))

colnames(Retail_Price_and_Dates_WeekDay)[2] <- "Mean_Retail_Price"

```

<center>

```{r 6, echo=FALSE}

# Retail Price By Month
RetailPricePlot1 <- ggplotly(
  ggplot(Retail_Price_and_Dates_Month) +
 aes(x = Date, y = Mean_Retail_Price) +
 geom_line(size = 1.1, 
 colour = "#1c6155") + geom_point() +
 labs(x = "Months", y = "Average Retail Price", title = "Average Retail Price of Computer in 2018 by Months", 
 subtitle = "Aggregated by Month") +
 theme_classic()
 )

RetailPricePlot1

# Retail Price By Week
RetailPricePlot2 <- ggplotly(
  ggplot(Retail_Price_and_Dates_Week) +
 aes(x = Date, y = Mean_Retail_Price) +
 geom_line(size = 0.4, 
 colour = "#1c6155") + geom_point() +
 labs(x = "Weeks", y = "Average Retail Price", title = "Average Retail Price of Computer in 2018 by Weeks", 
 subtitle = "Aggregated by Week") +
 theme_classic()
 )

RetailPricePlot2

# Retail Price By Day
RetailPricePlot3 <- ggplotly(
  ggplot(Retail_Price_and_Dates_Day) +
 aes(x = Date, y = Mean_Retail_Price) +
 geom_line(size = 0.2, 
 colour = "#1c6155")  +
 labs(x = "Days", y = "Average Retail Price", title = "Average Retail Price of Computer in 2018 by Days", 
 subtitle = "Aggregated by Day") + 
 theme_classic()
 )

RetailPricePlot3

# Retail Price By Week Day

RetailPricePlot4 <- ggplotly(
  ggplot(Retail_Price_and_Dates_WeekDay) +
 aes(x = Translation, y = Mean_Retail_Price) +
 geom_col(fill = "#1c6155") +
 labs(x = "Weekdays", y = "Average Retail Price", title = "Average Retail Price In 2018 by Weekdays", subtitle = "In 2018") + 
 theme_classic() + coord_cartesian(ylim = c(505, 510))
)

RetailPricePlot4

```

</center>

> Those Plots show different aggregations levels, can be used depending on the analysis we want, thus the granularity need. End of the weekdays is generally having higher retail prices, such as the year starting with month period from May to December. 

### iii. Are prices consistent across retail outlets?


```{r 7, include=FALSE}

# Set Up a Data Subset and NA OMIT

Retail_Price_Outlets_Date <- Laptop_Sales_Data[,.(Retail.Price,Store.Postcode,Date)][,Date:=mdy_hm(Date)]

Retail_Price_Outlets_Date  <- na.omit(Retail_Price_Outlets_Date)

Retail_Price_Configuration <- Laptop_Sales_Data[,.(Retail.Price,Configuration,Screen.Size..Inches.,Battery.Life..Hours.,RAM..GB.,Processor.Speeds..GHz.,Integrated.Wireless.,HD.Size..GB.,Bundled.Applications.)]

Retail_Price_Configuration <- na.omit(Retail_Price_Configuration)

```


<center>


```{r 8,echo=FALSE}

# Boxplot Across Retail Outlets

ggplotly(
ggplot(Retail_Price_Outlets_Date) +
 aes(x = Store.Postcode, y = Retail.Price) +
 geom_boxplot(fill = "#1c6155") +
 labs(x = "Stores Postcode", y = "Price", title = "Boxplot Of The Retail Price Across Stores", subtitle = "In 2018") +
 theme_classic() + scale_x_discrete() + theme(axis.text.x=element_text(size=rel(1), angle=90))
)

```

</center>

> Each box plots belongs to a specific stores, we can see a common trend across all stores in 2018. We also see that 5 stores tend to have a lower retail price than others with the median closer to 465.


```{r 81, include=FALSE}

# Retail Price Across Stores During 2018 - Data

Retail_Price_Outlets_Date_Month <- Retail_Price_Outlets_Date[,Floor.Date:=floor_date(Date,unit="month")][,c(Mean_Price=mean(Retail.Price)), by=list(Store.Postcode,Floor.Date)]

colnames(Retail_Price_Outlets_Date_Month)[3] <- "Mean_Retail_Price"

```

<center>


```{r 83, echo=FALSE}

# Plot of the Monthly Retail Price per Stores

ggplotly(
ggplot(Retail_Price_Outlets_Date_Month) +
 aes(x = Floor.Date, y = Mean_Retail_Price, colour = Store.Postcode) +
 geom_line(size = 0.5) +
 scale_color_hue(direction = 1) +
 labs(x = "Month", y = "Price", title = "Retail Price Across Months and Grouped by Stores", 
 subtitle = "In 2018") +
 theme_classic() 
)

```

</center>

> Looking at times series, we can see that not all stores have the same time trend, but most of them do.


### iv. How does price change with configuration?


<center>

```{r 82, echo=FALSE, warning=FALSE, comment=FALSE, results = FALSE} 

# Plot Of The Retail Price per Configuration

ggplot(Retail_Price_Configuration) +
 aes(x = Configuration, y = Retail.Price) +
 geom_point(shape = "circle", size = 0.6) +
 scale_color_gradient() +
 labs(y = "Retail Price", title = "Retail Price and Configuration ", 
 subtitle = "In 2018") + geom_smooth(color='#1c6155') + theme_classic()

``` 

</center>

> Using an smooth approximator, we can see two differents trends, first a rapid increase in price while being at low configurations, and then the slope tend to stay constant and low, ending with a increase with highest configurations. 


## b.Location Questions

### i. Where are the stores and customers locatd?


```{r 9, echo=FALSE} 

# Transform UK to Worldwide Geodata

## For Client Data

Data_Client_Coordinates <- Laptop_Sales_Data[,.(customer.X,customer.Y)]
Data_Client_Coordinates <- na.omit(Data_Client_Coordinates)
Data_Client_Coordinates <- distinct(Data_Client_Coordinates)
Data_Client_Coordinates <- transform_coordinates(Data_Client_Coordinates,
                                                        latitude="customer.Y",longitude = "customer.X", from = projection_bng(), to = projection_wgs84())
Data_Client_Coordinates$customer.X <- as.numeric(Data_Client_Coordinates$customer.X)
Data_Client_Coordinates$customer.Y <- as.numeric(Data_Client_Coordinates$customer.Y)

## For Store Data

Data_Stores_Coordinates <- Laptop_Sales_Data[,.(store.X,store.Y)]
Data_Stores_Coordinates <- na.omit(Data_Stores_Coordinates)
Data_Stores_Coordinates <- distinct(Data_Stores_Coordinates)
Data_Stores_Coordinates <- transform_coordinates(Data_Stores_Coordinates,
                                                        latitude="store.Y",longitude = "store.X", from = projection_bng(), to = projection_wgs84())

Data_Stores_Coordinates$store.X <- as.numeric(Data_Stores_Coordinates$store.X)
Data_Stores_Coordinates$store.Y <- as.numeric(Data_Stores_Coordinates$store.Y)


```

<center>

```{r 84,echo=FALSE} 

# Plotting Map with leafleet

map_1 <- leaflet() %>% addProviderTiles(providers$CartoDB.Positron) %>% addMarkers(data=Data_Client_Coordinates, lng = ~Data_Client_Coordinates$customer.X, lat = ~Data_Client_Coordinates$customer.Y,icon = list(iconUrl='https://cdn-icons-png.flaticon.com/512/4573/4573516.png',iconSize=c(25,25)), clusterOptions = markerClusterOptions(),popup = ~paste("<h3>Client Coordinates</h3>","<b>Latitude:</b>",customer.X,"<b>Longitude:</b>",customer.Y)) %>% addMarkers(data=Data_Stores_Coordinates, lng = ~Data_Stores_Coordinates$store.X, lat = ~Data_Stores_Coordinates$store.Y,icon = list(iconUrl='https://cdn-icons-png.flaticon.com/512/726/726569.png',iconSize=c(25,25)), clusterOptions = markerClusterOptions(), popup = ~paste("<h3>Store Coordinates</h3>","<b>Latitude:</b>",store.X,"<b>Longitude:</b>",store.Y))

# Calling Leafleet stored map

map_1

```

</center>

> Enjoy looking at each stores and customers in London UK ! You can find there exact location by clicking on them ! We can see a big cluster of 545 clients/stores in the center of City London.

> transform_coordinates: Is a convinient function from Gissr (on Gihtub) that use the cran-project SpTransform as source code but can directly use coordinates in a dataframe and return it in a dataframe. The spTransform methods provide transformation between datum(s) and conversion between projections (also known as projection and/or re-projection), from one unambiguously specified coordinate reference system (CRS) to another, prior to version 1.5 using Proj4 projection arguments.


### ii. Which stores are selling the most?


```{r 10, include=FALSE}

# Data Preparation

Laptop_Sales_Data_2 <- Laptop_Sales_Data

Sales_Stores <- Laptop_Sales_Data_2[, .N, by=Store.Postcode]

Sales_Stores_2 <- Laptop_Sales_Data_2[,.(Revenues=sum(Retail.Price, na.rm = TRUE)),by=Store.Postcode]

```


<center>

```{r 103, echo=FALSE}

# Plot Transactions per Stores

ggplotly(
ggplot(Sales_Stores) +
 aes(x = reorder(Store.Postcode,-N), y = N) +
 geom_col(fill = "#1c6155") +
 labs(x = "Stores", 
 y = "Number Of Transactions", title = "Number of Transactions per Store", subtitle = "In 2018") +
 theme_classic() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
)

# Plot Revenues per Stores

ggplotly(
ggplot(Sales_Stores_2) +
 aes(x = reorder(Store.Postcode,-Revenues), y = Revenues) +
 geom_col(fill = "#1c6155") +
 labs(x = "Store Postcode", 
 y = "Revenues", title = "Revenues per Stores", subtitle = "In 2018") + 
 theme_classic()+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
)

```

</center>

> The following histograms show two ways of analyzing the stores sales results: by the number of transactions or the sales revenues they each generated during 2018. The Store SW1P 3AU sold the most and with the highest revenues. 


### iii. How far would customers travel to buy a laptop ?


<center>

```{r 11134,echo=FALSE,warning=FALSE,message=FALSE,fig.width=12}

#Transform UK to Worldwide Geodata

## For Client Data

Data_Client_Coordinates_Group <- Laptop_Sales_Data[,.(customer.X,customer.Y,Store.Postcode)]
Data_Client_Coordinates_Group <- na.omit(Data_Client_Coordinates_Group)
Data_Client_Coordinates_Group <- distinct(Data_Client_Coordinates_Group)
Data_Client_Coordinates_Group <- transform_coordinates(Data_Client_Coordinates_Group,
                                                        latitude="customer.Y",longitude = "customer.X", from = projection_bng(), to = projection_wgs84())
Data_Client_Coordinates_Group$customer.X <- as.numeric(Data_Client_Coordinates_Group$customer.X)
Data_Client_Coordinates_Group$customer.Y <- as.numeric(Data_Client_Coordinates_Group$customer.Y)

## For Store Data

Data_Stores_Coordinates_Group <- Laptop_Sales_Data[,.(store.X,store.Y,Store.Postcode)]
Data_Stores_Coordinates_Group <- na.omit(Data_Stores_Coordinates_Group)
Data_Stores_Coordinates_Group <- distinct(Data_Stores_Coordinates_Group)
Data_Stores_Coordinates_Group <- transform_coordinates(Data_Stores_Coordinates_Group,
                                                        latitude="store.Y",longitude = "store.X", from = projection_bng(), to = projection_wgs84())

Data_Stores_Coordinates_Group$store.X <- as.numeric(Data_Stores_Coordinates_Group$store.X)
Data_Stores_Coordinates_Group$store.Y <- as.numeric(Data_Stores_Coordinates_Group$store.Y)

##  Map of Clients and Stores with Historical Transactions Clusters, for Distances

ggplot()+geom_point(data=Data_Client_Coordinates_Group, aes(customer.X,customer.Y,color=Store.Postcode,fill=Store.Postcode),size=0.8,shape=21) + geom_point(data=Data_Stores_Coordinates_Group, aes(store.X,store.Y,color=Store.Postcode,fill=Store.Postcode),size=3,shape=24)+ggtitle("Clients and Stores Distances, with Past Transactions Clusters")+xlab("Latitude")+ylab("Longitutde")+theme_classic()


```

</center>


> With this plot we can see the distance between Customers and Stores in terms of latitude and longitude.



### iv. How far would customers travel to buy a laptop ? - Alternative


```{r 111, include=FALSE}

# Preparing the Dataset

Distance_Customer_Shop_Data <- Laptop_Sales_Data[, .(customer.X,customer.Y,store.X,store.Y)]

Distance_Customer_Shop_Data <- distinct(Distance_Customer_Shop_Data)

Distance_Customer_Shop_Data <- na.omit(Distance_Customer_Shop_Data)

Distance_Customer_Shop_Data$customer.X <- as.numeric(Distance_Customer_Shop_Data$customer.X)
Distance_Customer_Shop_Data$customer.Y <- as.numeric(Distance_Customer_Shop_Data$customer.Y)
Distance_Customer_Shop_Data$store.X <- as.numeric(Distance_Customer_Shop_Data$store.X)
Distance_Customer_Shop_Data$store.Y <- as.numeric(Distance_Customer_Shop_Data$store.Y)

## Transforming Coordinates in Worlwide standards

Distance_Customer <- transform_coordinates(Distance_Customer_Shop_Data, latitude="customer.Y",longitude = "customer.X", from = projection_bng(), to = projection_wgs84())

Distance_Customer <- Distance_Customer[,c(1,2)]

Distance_Store <- transform_coordinates(Distance_Customer_Shop_Data, latitude="store.Y",longitude = "store.X", from = projection_bng(), to = projection_wgs84())

Distance_Store <- Distance_Store[,c(3,4)]

# Append Distance Column

All_Distance <- cbind(Distance_Customer,Distance_Store)

# Mutate and Compute Distances Column with Haversine

All_Distance <- All_Distance %>% rowwise() %>% mutate(Distance=distHaversine(c(customer.Y,customer.X),c(store.Y,store.X)))

```

> DistHarversine: The shortest distance between two points (i.e., the ’great-circle-distance’ or ’as the crow flies’),
according to the ’haversine method’. This method assumes a spherical earth, ignoring ellipsoidal
effects. The Haversine (’half-versed-sine’) formula was published by R.W. Sinnott in 1984, although it
has been known for much longer. At that time computational precision was lower than today (15
digits precision). With current precision, the spherical law of cosines formula appears to give
equally good results down to very small distances. 

```{r 1111,echo=FALSE}

# Show Render Table for Data Exploration

datatable(All_Distance, colnames = c('Customer X ', 'Customer Y', 'Store X', 'Store Y', 'Distance Between Them in Meters'))

```


> Each Unique Customer can be found here, swipe on the right and see the distance they need to travel to get to their store.

<center>

```{r 1121,echo=FALSE,message=FALSE,warning=FALSE}

# Histogram of Distances between Clients and Stores

ggplotly(
  
ggplot(All_Distance) +
 aes(x = Distance) +
 geom_histogram(bins = 60L, fill = "#1c6155") +
 labs(x = "Distance (in meters)",y = "Frequency", 
 title = "Histogram of the Distance between Clients and Stores", subtitle = "In 2018") +
 theme_classic() + geom_vline(aes(xintercept = median(All_Distance$Distance)),col='black',size=2) + annotate("text",  x=15000,y=200  ,                    # Add text for mean
           label = paste("Median =", round(median(All_Distance$Distance))),
           col = "black",
           size = 6)
)


```

</center>

> Histogram of the Distance between Clients and Stores, with median Distance being approximately 4203 meters.  

## c.Revenue Questions


### i. How do the sales volume in each store relate to Acell's revenues?


```{r 12, include=FALSE}

# Preparing Data for the Revenu Plot

Sales_Stores_Revenues <- Laptop_Sales_Data[,.(Revenues=sum(Retail.Price, na.rm = TRUE),Configuration), by= Store.Postcode]

Sales_Stores_Revenues$Revenues <- as.numeric(Sales_Stores_Revenues$Revenues)

Total_Revenue <- 72219555

Sales_Stores_Revenues <- Sales_Stores_Revenues[,Percentage_Revenue:=Revenues/Total_Revenue]

Sales_Stores_Revenues$Percentage_Revenue <- Sales_Stores_Revenues$Percentage_Revenue*100

# Preparing Data for the Revenues Plot

Sales_Stores_Revenues_2 <- Laptop_Sales_Data[,.(Revenues=sum(Retail.Price, na.rm = TRUE)), by= Store.Postcode]

Sales_Stores_Revenues_2$Revenues <- as.numeric(Sales_Stores_Revenues_2$Revenues)

Sales_Stores_Revenues_2 <- Sales_Stores_Revenues_2[,Percentage_Revenue:=Revenues/Total_Revenue]

Sales_Stores_Revenues_2$Percentage_Revenue <- Sales_Stores_Revenues_2$Percentage_Revenue*100

```


<center>

```{r 121,echo=FALSE}

# Proportional Revenues participation of each stores 

ggplotly(
  ggplot(Sales_Stores_Revenues_2) +
 aes(x = reorder(Store.Postcode,-Percentage_Revenue), y = Percentage_Revenue) +
 geom_col(fill = "#1c6155") +
 labs(x = "Store Postcode", y = "% of Total Revenues", title = "Revenues Contribution per Stores", 
 subtitle = "In 2018") + 
 theme_classic()+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
 )

```

</center>

> You can see the proportional revenues participation of each stores in 2018. SW1P 3AU still is the store contributing the most to Acell's Revenues.


### ii. How does this relationship depend on the configuration?


<center>

```{r 1313, echo=FALSE}

# Relationship with configuration plot and stores

ggplot(Sales_Stores_Revenues) +
 aes(x = Configuration, y = Store.Postcode, fill = Percentage_Revenue) +
 geom_boxplot() +
 scale_fill_gradient(low = "#F7FCFD", high = "#1c6155") +
 labs(y = "Stores", title = "Boxplots of Configurations amongst Stores", 
 subtitle = "In 2018", fill = "% Total Revenues") +
 theme_minimal()+coord_cartesian(xlim = c(180, 600))



```

</center>

> We can see that S1P 3AU propose higher configurations, while having the smallest % revenues participation out of the total revenues of the company, this could be because it sells higher priced configurations, thus selling less to customer during the year, only to a smaller client pool that wants a better PC for more productive computing work. 

## d.Configuration Questions 


### i. What are the details of each configuration? How does this relate to price?

<center>

```{r 14,echo=FALSE}

# Preparing Dataset

Detail_Price <- Laptop_Sales_Data[,.(Retail.Price,Screen.Size..Inches.,Battery.Life..Hours.,RAM..GB.,Processor.Speeds..GHz.,HD.Size..GB.,Integrated.Wireless.,Bundled.Applications.)]

Detail_Price <- na.omit(Detail_Price)

# Screen Size

detail_plot1 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Screen Size (Inches)", y = "Retail Price", title = "Retail Price compared to Screen Size", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Screen.Size..Inches.)) + theme(text = element_text(size = 6))    
 
# Battery Life

detail_plot2 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Battery Life (Hours)", y = "Retail Price", title = "Retail Price compared to Battery Life", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Battery.Life..Hours.)) + theme(text = element_text(size = 6))    

# RAM 

detail_plot3 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "RAM (GB)", y = "Retail Price", title = "Retail Price compared to RAM", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(RAM..GB.)) + theme(text = element_text(size = 6))   

# Processor Speed

detail_plot4 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Processor Speeds (GHz)", y = "Retail Price", title = "Retail Price compared to Processor Speed", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Processor.Speeds..GHz.)) + theme(text = element_text(size = 6))    

# HD Size

detail_plot5 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "HD Size (GB)", y = "Retail Price", title = "Retail Price compared to Hard Drive size", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(HD.Size..GB.)) + theme(text = element_text(size = 12))    

# Wireless

detail_plot6 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Integrated Wireless", y = "Retail Price", title = "Retail Price compared to Integrated Wireless", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Integrated.Wireless.)) + theme(text = element_text(size = 6))   

# Bundled Applications

detail_plot7 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Bundled Applications", y = "Retail Price", title = "Retail Price compared to Bundled Applications", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Bundled.Applications.)) + theme(text = element_text(size = 6))   

# All Together

grid.arrange(detail_plot1,detail_plot2,detail_plot3,detail_plot4,detail_plot6,detail_plot7)
grid.arrange(detail_plot5)

```

</center>

> Depending on the details of each configurations, we can see that some specs tend to increase the price higher, such as the screen size, high RAM and high battery life. 

### ii. Do all stores sell all configurations?


```{r 15, include=FALSE}

# Preparing Dataset

Stores_Details <- Laptop_Sales_Data[,.(Store.Postcode,Configuration)]

Stores_Details <- na.omit(Stores_Details)

Stores_Details$Configuration <- as.integer(Stores_Details$Configuration)

```


<center>

```{r 151,echo=FALSE, fig.width=6,fig.height=6}

# Plotting each configurations per stores

ggplotly(
ggplot(Stores_Details) +
 aes(x = Configuration) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 labs(x = "Stores ", 
 y = "Configurations Count", title = "Each Configurations per Stores", subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Store.Postcode), scales = "free")
)

```

</center>

> With this multiple facets barplots, you can spot which configuration is less or not sold depending on the store. S1P 3AU is not selling every configurations.


# <span style="color: #1c6155;">Ex 4.1</span> 

```{r a1, include=FALSE}

rm(list = ls()) # clean environment

```

## Loading Datas and Cleaning 

> Loading the dataset called "Cereals.csv" given for the Homework

```{r a2,echo=FALSE, warning=FALSE, comment=FALSE}

Cereals_Data <- fread("DATA/Cereals.csv")

#is.data.table(Cereals_Data)

str(Cereals_Data)

```


<center>


```{r a3, echo=FALSE, warning=FALSE, comment=FALSE}

# Plotting missing Variables

gg_miss_var(Cereals_Data, show_pct = TRUE)

```

</center>

> We can see that Carbo and Sugars are missing at level 1.3% (approx.) and Potass at level 2.6% (approx.)

## a.

> **Ordinal**: shelf, rating

> **Nominal**: name, mfr, type    

> **Quantitative/Numerical**: calories, protein, fat, sodium , sugars, potass, weight, cups, vitamins, fiber, carbo      


## b.

<center>

> Summary

```{r b1, echo=FALSE, warning=FALSE, comment=FALSE}

# Creating alternate dataset

Cereals_Data_2 <- Cereals_Data

Cereals_Data_3 <- Cereals_Data

Cereals_Data <- na.omit(Cereals_Data)

b_result <- summary(Cereals_Data)

b_result

```


</center>

<center>

> Standard Errors 

```{r b11, echo=FALSE, warning=FALSE, comment=FALSE}

# Standard errors computations

b_sd <- sapply(Cereals_Data,sd,na.rm=TRUE)

b_sd

```

</center>


## c.

<center>

> Histogram of Quantitative Variables

```{r b2, echo=FALSE, warning=FALSE, comment=FALSE}

# All Histogram creations

Histo_Cereals_Data <- Cereals_Data_2[,c("calories","protein","fat","sodium" ,"sugars","potass","weight","cups","vitamins","fiber","carbo")]

Histo_Graph_1 <- ggplot(Histo_Cereals_Data) +
 aes(x = calories) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_2 <- ggplot(Histo_Cereals_Data) +
 aes(x = protein) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_3 <- ggplot(Histo_Cereals_Data) +
 aes(x = sugars) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_4 <- ggplot(Histo_Cereals_Data) +
 aes(x = potass) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_5 <- ggplot(Histo_Cereals_Data) +
 aes(x = weight) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_6 <- ggplot(Histo_Cereals_Data) +
 aes(x = cups) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_7 <- ggplot(Histo_Cereals_Data) +
 aes(x = vitamins) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_8 <- ggplot(Histo_Cereals_Data) +
 aes(x = fiber) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_9 <- ggplot(Histo_Cereals_Data) +
 aes(x = carbo) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_10 <- ggplot(Histo_Cereals_Data) +
 aes(x = sodium) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_11 <- ggplot(Histo_Cereals_Data) +
 aes(x = fat) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

```

</center>

<center>


```{r b3, echo=FALSE, warning=FALSE, comment=FALSE}

# Arranging Histograms in one plot

grid.arrange(Histo_Graph_1,Histo_Graph_2,Histo_Graph_3,Histo_Graph_4,Histo_Graph_5,Histo_Graph_6,Histo_Graph_7,Histo_Graph_8,Histo_Graph_9,Histo_Graph_10,Histo_Graph_11)

```

</center>

<center>

> Standards Errors

```{r b4, echo=FALSE, warning=FALSE, comment=FALSE}

# Standard errors computing

quantitative_sd <- sapply(Histo_Cereals_Data,sd,na.rm=TRUE)

quantitative_sd

```

</center>

### i. Which variables have the largest variability?

> Based on the Histogram Grid and the Standard Errors Summary, Sodium, Potass and Vitamins have the largest variability.

### ii. Which variables seem skedew?

> Potassium, Fiber and Fat seem skewed. Cups could also be. 

### iii. Are there any values that seem extreme? 

> We can see that Fiber has at least 3 extremes values (2 classes away from the main cluster) have extremes values. We could check with some boxplots to better see what are those outliers. 

<center> 

> Multiple Boxplots for outliers detections

```{r c1586, echo=FALSE, warning=FALSE, comment=FALSE}

# Boxplot for Extremes values


box1 <- ggplotly(ggplot(Cereals_Data_2) +
 aes(x = "", y = fiber) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Fiber") +
 theme_classic())

box2 <- ggplotly(ggplot(Cereals_Data_2) +
 aes(x = "", y = calories) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Calories") +
 theme_classic())

box3 <- ggplotly(ggplot(Cereals_Data_2) +
 aes(x = "", y = vitamins) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Vitamins") +
 theme_classic())

box4 <- ggplotly(ggplot(Cereals_Data_2) +
 aes(x = "", y = weight) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Weight") +
 theme_classic())

box5 <- ggplotly(ggplot(Cereals_Data_2) +
 aes(x = "", y = cups) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Cups") +
 theme_classic())

box6 <- ggplotly(ggplot(Cereals_Data_2) +
 aes(x = "", y = potass) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Potass") +
 theme_classic())

box7 <- ggplotly(ggplot(Cereals_Data_2) +
 aes(x = "", y = protein) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Protein") +
 theme_classic())

box8 <- ggplotly(ggplot(Cereals_Data_2) +
 aes(x = "", y = sodium) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Sodium") +
 theme_classic())


# Combine all Plotly Boxplots together for keeping interactivity 

combineWidgets(box1,box2,box3,box4,box5,box6,box7,box8,nrow = 2)

```


</center> 


## d.

<center>

```{r d, echo=FALSE, warning=FALSE, comment=FALSE}

# Plotting Calories in Hot vs Cold Cereals

ggplotly(
ggplot(Cereals_Data_3) +
 aes(x = type, y = calories) +
 geom_boxplot(fill = "#1c6155") +
 labs(x = "Type", 
 y = "Calories", title = "Calories in Hot VS Cold Cereals", subtitle = "Side-by-side Boxplots") +
 theme_classic()
)

```

</center>

> We are lacking data about Hot Type Cereals to compare both state of cereals. 

## e.

<center>

```{r e, echo=FALSE, warning=FALSE, comment=FALSE}

# Plotting Rating by Shelft Height

ggplotly(
ggplot(Cereals_Data_3) +
 aes(x = "", y = rating) +
 geom_boxplot(fill = "#1c6155") +
 labs(x = "Shelf Height", 
 y = "Rating", title = "Rating grouped by Shelf Height", subtitle = "Side-by-side Boxplots") +
 theme_classic() +
 facet_wrap(vars(shelf))
)

```

</center>

> Shelf 1 and 3 are pretty close (both median close to 40-42), we could use a statistical test for comparing the three boxplots and see if there is a real median/mean differences. Without doing any statistical test, we can also see that the boxplot medians are overlapping for category 1 and 3, meaning we could interpret them as identical groups on average. 



## f.

### i. Which pair of variables is most strongly correlated?

<center>

> Correlation Matrix

```{r f, echo=FALSE, warning=FALSE, comment=FALSE}

# Correlations matrices

Corr_Data <- Cereals_Data[,c("calories","protein","fat","sodium" ,"sugars","potass","weight","cups","vitamins","fiber","carbo")]

Corr_Data <- na.omit(Corr_Data)

Corr_plot <- cor(Corr_Data)

Corr_plot

```


</center>


<center>


```{r f1, echo=FALSE, warning=FALSE, comment=FALSE}

# Correlations plotting

corrplot(Corr_plot, method = "color", col=brewer.pal(n=8, name="BuGn"),tl.col="black",tl.srt=45,addCoef.col = "black",number.cex = 0.5)

```

</center>

> Fiber and Potass seems to have a strong correlation

### ii. How can we reduce the number of variables based on these correlations? 

</center>

> We could select the highest correlated variable (because of threat of multicollinearity) and removed them. In the context of a Regression, using VIF on our model would suggest us which explanatory variabes we should remove based on those correlations table. 

### iii. How would the correlations change if we normalized the data first?

<center>

> Correlation Matrix

```{r f5, echo=FALSE, warning=FALSE, comment=FALSE}

# Normalization of Data with Scale

Scale_data <- as.data.frame(scale(Corr_Data))

Corr_plot_scale <- cor(Scale_data)

Corr_plot_scale

```

</center>

<center>


```{r f6, echo=FALSE, warning=FALSE, comment=FALSE}

# Correlation plot after Normalization

corrplot(Corr_plot_scale, method = "color", col=brewer.pal(n=8, name="PuBuGn"),tl.col="black",tl.srt=45,addCoef.col = "black",number.cex = 0.5)

```

</center>

> Nothing changes when we normalized the data before correlation matrices and plots since normalization already occurs when computing correlations. 









# <span style="color: #1c6155;">References</span>

[Github Repo for this Homework 1](https://github.com/LiamPhan17/DATA_MINING_HW1)

[Data Mining for Business Analytics: Concepts, Techniques, and Applications in R](https://www.wiley.com/en-us/Data+Mining+for+Business+Analytics:+Concepts,+Techniques,+and+Applications+in+R-p-9781118879368)

[Gissr Github](https://github.com/skgrange/gissr/)

[Geosphere](https://cran.r-project.org/web/packages/geosphere/)

[DT](https://cran.r-project.org/web/packages/DT/index.html)

[Leaflet](https://leafletjs.com/)
